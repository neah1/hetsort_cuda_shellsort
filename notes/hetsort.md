## HET Sort optimization
- Splitting Strategy: It may be necessary to further split the dataset into chunks that can be sequentially processed by each GPU. This means each GPU might work on multiple chunks one after the other, effectively treating the GPU memory as a cache for parts of the dataset.
- Memory Management: The code allocates and frees GPU memory for each chunk. Depending on the size of your chunks and the available GPU memory, you might be able to optimize this by reusing allocated memory for multiple chunks.
- Asynchronous Execution: To fully leverage the capabilities of multiple GPUs, consider using CUDA streams for overlapping data transfers and computation, allowing different GPUs to work concurrently.
- Overlapping Computation and Transfer: When dealing with multiple chunks per GPU, you can overlap computation (sorting) on one chunk with the transfer of the next chunk to the GPU and the transfer of the previously sorted chunk back to the CPU. This requires careful management of CUDA streams and events to synchronize operations without stalling the GPU or CPU. 2N approach from Maltenberger and Stele & Jacobsen paper adapted for in-place sorting.
- Merge Logic: Implementing an efficient merge logic that takes advantage of early chunk availability can be complex, especially if chunks finish at vastly different times. You may need a more sophisticated data structure to track sorted chunks and their merge partners.
- Multiway merging: done at the end, multiple chunks, loser tree. Maltenberger's paper.
- Bi-directional copying inplaceMemCpy FROM MALTENBERGER (rtx 2080 has 2 copy engines) (pick optimal blocksize). 
- Eager merging according to Maltenberger is bad but make sure if its true for us as well.
- Allocate buffer to thrustsort to make use of 2n sorting.
- Group chunks into chunk groups to distribute to GPUs. 
- Deal with GPU variable free memory.
- Implement ThreadPool for each GPU (with Pragma optimization - OpenMP).
- openmp for fetching gpu data
- 2n, inplace, etc. Different sorters to test thrustsort vs shellsort
- Faster mem transfer by splitting array and using multiple streams. Double copy (from inplace)
- Thrustsort needs a bigger buffer than data. Around 1.2-1.5 buffer size compared to sorted array. This means that inplace solutions allow for bigger chunkSize than 2N given same amount of freeMem.
- Add extra mem for thrustsort
- Fix inplace: SOME ERRORS APPEAR RANDOMLY AND THEN GO AWAY WITHOUT ANY CHANGES THE NEXT DAY. PROBABLY DUE TO SOME EXTERNAL VARIABLES IN THE HPC.
- Optimize chunk so that every pass => same chunk size (NO. small chunks bad)
- Time to retrieve freeMem info from GPU varies from machine to machine (and daily). Also external factor.
- Instead of inplace, double stream speed? Inplace was wonky to begin with.
- 3n Thrustsorts